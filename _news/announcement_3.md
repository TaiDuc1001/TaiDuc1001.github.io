---
layout: post
title: üì∞ 02 papers is accepted at The 25th Asia-Pacific Network Operations and Management Symposium (APNOMS 2025), Kaohsiung, Taiwan.
date: 2025-08-02
inline: false
related_posts: false
---
Our paper titled `"ALMUS: Enhancing Active Learning for Object Detection with Metric-Based Uncertainty Sampling"` has been accepted for presentation at the 25th Asia-Pacific Network Operations and Management Symposium (APNOMS 2025) in Kaohsiung, Taiwan. This work introduces ALMUS, a novel active learning acquisition strategy that adapts dynamically to different dataset distributions and annotation budgets for object detection tasks. ALMUS combines three acquisition modes‚Äîuncertainty sampling, diversity-based coreset selection, and class distribution balancing‚Äîinto a unified framework capable of switching modes during the learning process. Evaluations on traffic datasets demonstrate that ALMUS consistently achieves higher mean Average Precision (mAP) with fewer labeled samples compared to single-strategy baselines. The results highlight its potential to reduce labeling costs while maintaining high model performance, making it particularly suitable for real-world intelligent transportation systems and other resource-constrained applications.

   - `Duc Tai Phan`, <a href='https://nhut-ngnn.github.io/'>Nhut Minh Nguyen</a>, Khang Phuc Nguyen, Tri Pham, and <a href='https://dnmduc.github.io/'>Dr. Duc Ngoc Minh Dang</a>, `‚ÄúALMUS: Enhancing Active Learning for Object Detection with Metric-Based Uncertainty Sampling‚Äù`, <a href='https://sites.google.com/view/apnoms2025'>The 25th Asia-Pacific Network Operations and Management Symposium (APNOMS 2025)</a>, Kaohsiung, Taiwan

Our paper titled `"CemoBAM: Advancing Multimodal Emotion Recognition through Heterogeneous Graph Networks and Cross-Modal Attention Mechanisms"` has been accepted for presentation at the 25th Asia-Pacific Network Operations and Management Symposium (APNOMS 2025) in Kaohsiung, Taiwan. This work introduces CemoBAM, a novel dual-stream architecture combining a Cross-modal Heterogeneous Graph Attention Network (CH-GAT) with a Cross-modal Convolutional Block Attention Mechanism (xCBAM) to enhance speech emotion recognition from audio and text. By capturing fine-grained intra- and inter-modal relationships and emphasizing emotionally salient features, CemoBAM achieves state-of-the-art performance on the IEMOCAP and ESD datasets, surpassing previous methods by 0.32% and 3.25%, respectively. The results underscore the effectiveness of integrating graph-based reasoning with attention-based feature refinement for robust multimodal emotion recognition.

   - <a href='https://nhut-ngnn.github.io/'>Nhut Minh Nguyen</a>, Thu Thuy Le, Thanh Trung Nguyen, `Duc Tai Phan`, Anh Khoa Tran, and <a href='https://dnmduc.github.io/'>Dr. Duc Ngoc Minh Dang</a>, `‚ÄúCemoBAM: Advancing Multimodal Emotion Recognition through Heterogeneous Graph Networks and Cross-Modal Attention Mechanisms‚Äù`, <a href='https://sites.google.com/view/apnoms2025'>The 25th Asia-Pacific Network Operations and Management Symposium (APNOMS 2025)</a>, Kaohsiung, Taiwan